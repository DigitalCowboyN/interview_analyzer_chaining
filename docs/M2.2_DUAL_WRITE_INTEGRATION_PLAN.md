# M2.2: Dual-Write Integration Plan

## Overview

Integrate event emission into the existing pipeline alongside Neo4j writes. The pipeline will continue to write to Neo4j directly (for stability) while also emitting events to EventStoreDB (for the new event-sourced architecture).

## Current Architecture Analysis

### Pipeline Flow

1. **File Upload** → `PipelineOrchestrator._process_single_file()`

   - Generates project_id and interview_id (via test override of `_setup_file_io`)
   - Creates `Neo4jMapStorage` and `Neo4jAnalysisWriter` with these IDs

2. **Sentence Creation** → `Neo4jMapStorage.write_entry()`

   - Writes sentences to Neo4j
   - Creates `Project`, `Interview`, `Sentence` nodes
   - Links via `CONTAINS_INTERVIEW` and `HAS_SENTENCE`

3. **Analysis Generation** → `Neo4jAnalysisWriter.write_result()`

   - Writes analysis results to Neo4j
   - Creates `Analysis` nodes, dimension nodes (Keywords, Topics, etc.)
   - Links via `HAS_ANALYSIS`, `MENTIONS_*` relationships

4. **Graph Persistence** → `save_analysis_to_graph()`
   - Adds `PART_OF_FILE` and `FOLLOWS` relationships

### Key Integration Points

| Integration Point     | Current Behavior                | Event to Emit       | Aggregate |
| --------------------- | ------------------------------- | ------------------- | --------- |
| File processing start | Creates Project/Interview nodes | `InterviewCreated`  | Interview |
| Sentence creation     | Creates Sentence node           | `SentenceCreated`   | Sentence  |
| Analysis generation   | Creates Analysis node           | `AnalysisGenerated` | Sentence  |

### Critical Requirements

1. **Same IDs Everywhere**

   - `interview_id` must be identical in Neo4j and EventStoreDB
   - `sentence_id` must be identical in Neo4j and EventStoreDB
   - These are currently generated as UUIDs in tests, but `sentence_id` is an integer (sequence_order)

2. **Non-Blocking**

   - Event emission must NOT block Neo4j writes
   - Neo4j write failure should NOT be affected by event failures
   - Log event failures but continue processing

3. **Actor Tracking**

   - System-generated events: `Actor(actor_type=ActorType.SYSTEM, user_id="pipeline")`
   - Include source="pipeline" in event envelope

4. **Correlation IDs**
   - Generate `correlation_id` per file (interview)
   - All events for same file share same `correlation_id`
   - Track causation chain: `InterviewCreated` → `SentenceCreated` → `AnalysisGenerated`

---

## Implementation Strategy

### Phase 1: Command Infrastructure for Pipeline

**Goal:** Create a thin command layer that wraps the existing pipeline logic

**Components:**

- `src/commands/pipeline_commands.py` - Commands for file processing
- `src/commands/pipeline_handler.py` - Handler that emits events
- `src/pipeline_event_emitter.py` - Event emission wrapper

**Approach:**

1. **DO NOT modify command handlers** - They're for user-driven commands
2. **Create separate pipeline event emitter** - Direct event creation (no commands)
3. **Inject emitter into pipeline** - Pass as optional parameter

---

### Phase 2: Event Emission Points

#### 2.1: InterviewCreated Event

**Location:** `PipelineOrchestrator._process_single_file()` after `_setup_file_io()`

**Trigger:** When file processing starts

**Event Data:**

```python
{
    "project_id": project_id,
    "title": file_path.name,  # Use filename as title
    "source": str(file_path),
    "language": "en",  # Could detect or default
    "status": "processing"
}
```

**Implementation:**

```python
# After: data_source, map_storage, analysis_writer, _ = self._setup_file_io(file_path)
if self.event_emitter:
    await self.event_emitter.emit_interview_created(
        interview_id=analysis_writer.interview_id,
        project_id=analysis_writer.project_id,
        title=file_name,
        source=str(file_path),
        correlation_id=self.correlation_id  # Generated per file
    )
```

---

#### 2.2: SentenceCreated Events

**Location:** `Neo4jMapStorage.write_entry()` after successful Neo4j write

**Trigger:** When sentence is written to Neo4j

**Event Data:**

```python
{
    "interview_id": self.interview_id,
    "index": entry["sequence_order"],
    "text": entry["sentence"],
    "speaker": entry.get("speaker"),  # If available
    "start_ms": None,  # Not available in text pipeline
    "end_ms": None
}
```

**Implementation:**

```python
# In Neo4jMapStorage.__init__():
self.event_emitter = event_emitter  # Inject from orchestrator

# In Neo4jMapStorage.write_entry() after successful Neo4j write:
if self.event_emitter:
    await self.event_emitter.emit_sentence_created(
        sentence_id=str(uuid.uuid4()),  # Generate UUID for event store
        interview_id=self.interview_id,
        index=entry["sequence_order"],
        text=entry["sentence"],
        correlation_id=self.correlation_id
    )
```

**CRITICAL ISSUE:** `sentence_id` mismatch

- Neo4j uses `sentence_id` as integer (sequence_order: 0, 1, 2, ...)
- EventStoreDB aggregate_id must be UUID string
- **Solution:** Use interview_id + sequence_order as composite key
  - Store in event data: `{"interview_id": "...", "index": 0}`
  - Use deterministic UUID: `uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:{index}")`

---

#### 2.3: AnalysisGenerated Events

**Location:** `Neo4jAnalysisWriter.write_result()` after successful Neo4j write

**Trigger:** When analysis is written to Neo4j

**Event Data:**

```python
{
    "model": result.get("model", "gpt-4"),
    "model_version": result.get("model_version", "1.0"),
    "classification": {
        "function_type": result.get("function_type"),
        "structure_type": result.get("structure_type"),
        "purpose": result.get("purpose")
    },
    "keywords": result.get("overall_keywords", []),
    "topics": result.get("topics", []),
    "domain_keywords": result.get("domain_keywords", []),
    "confidence": result.get("confidence", 1.0)
}
```

**Implementation:**

```python
# In Neo4jAnalysisWriter.__init__():
self.event_emitter = event_emitter  # Inject from orchestrator

# In Neo4jAnalysisWriter.write_result() after successful Neo4j write:
if self.event_emitter:
    sentence_uuid = uuid.uuid5(
        uuid.NAMESPACE_DNS,
        f"{self.interview_id}:{result['sentence_id']}"
    )
    await self.event_emitter.emit_analysis_generated(
        sentence_id=str(sentence_uuid),
        interview_id=self.interview_id,
        analysis_data=result,
        correlation_id=self.correlation_id
    )
```

---

### Phase 3: Event Emitter Implementation

**File:** `src/pipeline_event_emitter.py`

```python
class PipelineEventEmitter:
    """
    Emits events to EventStoreDB during pipeline processing.

    Lightweight wrapper that creates events directly (no commands/aggregates).
    Used for dual-write phase to emit events alongside Neo4j writes.
    """

    def __init__(self, event_store_client: EventStoreClient):
        self.client = event_store_client
        self.logger = get_logger()

    async def emit_interview_created(
        self,
        interview_id: str,
        project_id: str,
        title: str,
        source: str,
        correlation_id: str
    ):
        """Emit InterviewCreated event (non-blocking)."""
        try:
            event = create_interview_created_event(
                interview_id=interview_id,
                data={
                    "project_id": project_id,
                    "title": title,
                    "source": source,
                    "language": "en",
                    "status": "processing"
                },
                actor=Actor(actor_type=ActorType.SYSTEM, user_id="pipeline"),
                correlation_id=correlation_id,
                project_id=project_id
            )

            stream_name = f"Interview-{interview_id}"
            await self.client.append_events(stream_name, [event], expected_version=-1)
            self.logger.debug(f"Emitted InterviewCreated event for {interview_id}")

        except Exception as e:
            # Log but don't raise - Neo4j write already succeeded
            self.logger.error(f"Failed to emit InterviewCreated event: {e}", exc_info=True)

    # Similar methods for sentence_created, analysis_generated...
```

---

### Phase 4: Pipeline Integration

**Modifications:**

1. **`PipelineOrchestrator.__init__()`:**

   ```python
   # Add event emitter initialization
   self.event_emitter = None
   if config.get("event_sourcing", {}).get("enabled", False):
       event_store = EventStoreClient(...)
       self.event_emitter = PipelineEventEmitter(event_store)
   ```

2. **`PipelineOrchestrator._setup_file_io()`:**

   ```python
   # Inject emitter into storage components
   map_storage = Neo4jMapStorage(project_id, interview_id)
   map_storage.event_emitter = self.event_emitter
   map_storage.correlation_id = self.correlation_id  # Generated per file

   analysis_writer = Neo4jAnalysisWriter(project_id, interview_id)
   analysis_writer.event_emitter = self.event_emitter
   analysis_writer.correlation_id = self.correlation_id
   ```

3. **`PipelineOrchestrator._process_single_file()`:**

   ```python
   # Generate correlation_id for this file
   self.correlation_id = str(uuid.uuid4())

   # After _setup_file_io():
   if self.event_emitter:
       await self.event_emitter.emit_interview_created(...)
   ```

---

### Phase 5: Configuration & Feature Flags

**Environment Variables:**

```bash
# Feature flag
ENABLE_EVENT_SOURCING=true

# EventStoreDB connection (already configured)
ESDB_CONNECTION_STRING=esdb://localhost:2113?tls=false
```

**Config Structure:**

```python
config = {
    "event_sourcing": {
        "enabled": os.getenv("ENABLE_EVENT_SOURCING", "false").lower() == "true",
        "connection_string": os.getenv("ESDB_CONNECTION_STRING"),
        "emit_on_failure": False,  # Don't emit if Neo4j write fails
    }
}
```

---

### Phase 6: Error Handling & Metrics

**Error Handling:**

1. **Neo4j write succeeds, event emission fails:**

   - Log error with full context
   - Continue processing (Neo4j has data)
   - Track metric: `event_emission_failures_total`

2. **Neo4j write fails:**

   - Do NOT emit event
   - Existing error handling continues

3. **Event append succeeds but wrong version:**
   - Log warning (shouldn't happen in dual-write)
   - Track metric: `event_version_conflicts_total`

**Metrics to Add:**

- `events_emitted_total{event_type}` - Counter
- `event_emission_duration_seconds{event_type}` - Histogram
- `event_emission_failures_total{event_type, reason}` - Counter
- `dual_write_success_rate` - Gauge (Neo4j success + Event success)

---

## Implementation Order

1. ✅ **Analysis Complete** - Understand integration points
2. ⏳ **Create PipelineEventEmitter** - Event emission wrapper
3. ⏳ **Inject emitter into orchestrator** - Feature-flagged initialization
4. ⏳ **Add InterviewCreated emission** - File processing start
5. ⏳ **Add SentenceCreated emission** - Neo4jMapStorage integration
6. ⏳ **Add AnalysisGenerated emission** - Neo4jAnalysisWriter integration
7. ⏳ **Add error handling & metrics** - Monitoring and observability
8. ⏳ **Integration testing** - End-to-end validation

---

## Testing Strategy

**Unit Tests:**

- Test `PipelineEventEmitter` methods (mocked EventStoreClient)
- Test event creation with correct data
- Test error handling (doesn't raise)

**Integration Tests:**

- Test full pipeline with event emitter enabled
- Verify events in EventStoreDB after pipeline run
- Verify Neo4j data unchanged
- Test event emission failure doesn't break pipeline

**Success Criteria:**

- Pipeline runs successfully with events enabled
- All events appear in EventStoreDB streams
- Neo4j data identical to non-event mode
- Event emission failures logged but don't break pipeline

---

## Risks & Mitigation

| Risk                            | Impact | Mitigation                                 |
| ------------------------------- | ------ | ------------------------------------------ |
| Event emission slows pipeline   | Medium | Make async, use asyncio.create_task()      |
| EventStoreDB connection failure | Low    | Catch exceptions, log, continue            |
| sentence_id mismatch            | High   | Use deterministic UUIDs (uuid5)            |
| correlation_id not tracked      | Medium | Generate per file, pass through storage    |
| Version conflicts in ESDB       | Low    | Use expected_version=-1 (allow duplicates) |

---

## Rollback Plan

If event emission causes issues:

1. Set `ENABLE_EVENT_SOURCING=false`
2. Pipeline continues with Neo4j only
3. No code changes needed
4. Can re-enable after fixing issues

---

## Next Steps

Continue with **Step 2: Create PipelineEventEmitter**
