# tests/io/test_neo4j_map_storage.py
"""
M2.8 Integration tests for Neo4j map storage with projection service.

Tests validate that:
1. write_entry() emits events correctly
2. Projection service processes events and writes to Neo4j
3. read_all_entries() reads from Neo4j correctly

Architecture:
  write_entry() → EventStoreDB → Projection Service → Neo4j → read_all_entries()
"""

import uuid
from typing import Any, Dict, List, Set

import pytest

from src.io.neo4j_map_storage import Neo4jMapStorage
from src.projections.handlers.interview_handlers import InterviewCreatedHandler
from src.projections.handlers.sentence_handlers import SentenceCreatedHandler
from src.pipeline_event_emitter import PipelineEventEmitter

# Mark all tests in this module as asyncio and require Neo4j + EventStore
pytestmark = [pytest.mark.asyncio, pytest.mark.integration, pytest.mark.neo4j, pytest.mark.eventstore]


async def process_events_through_projection(
    event_store,
    interview_id: str,
    num_sentences: int,
):
    """
    Helper to process events through projection service handlers.

    Args:
        event_store: EventStoreDB client
        interview_id: Interview UUID
        num_sentences: Number of sentence events to process
    """
    # Process InterviewCreated event
    interview_handler = InterviewCreatedHandler()
    interview_stream = f"Interview-{interview_id}"
    interview_events = await event_store.read_stream(interview_stream)

    for event in interview_events:
        if event.event_type == "InterviewCreated":
            await interview_handler.handle(event)

    # Process SentenceCreated events
    sentence_handler = SentenceCreatedHandler()

    for i in range(num_sentences):
        sentence_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:{i}"))
        sentence_stream = f"Sentence-{sentence_id}"
        sentence_events = await event_store.read_stream(sentence_stream)

        for event in sentence_events:
            if event.event_type == "SentenceCreated":
                await sentence_handler.handle(event)


# --- Tests for Neo4jMapStorage ---


async def test_neo4j_map_storage_init():
    """Tests basic initialization of Neo4jMapStorage."""
    project_id = "test-project-123"
    interview_id = "test-interview-456"

    storage = Neo4jMapStorage(project_id, interview_id)

    assert storage.project_id == project_id
    assert storage.interview_id == interview_id
    assert storage.get_identifier() == interview_id


async def test_neo4j_map_storage_init_empty_ids():
    """Tests that initialization raises ValueError for empty IDs."""
    with pytest.raises(ValueError, match="project_id and interview_id cannot be empty"):
        Neo4jMapStorage("", "interview-123")

    with pytest.raises(ValueError, match="project_id and interview_id cannot be empty"):
        Neo4jMapStorage("project-123", "")

    with pytest.raises(ValueError, match="project_id and interview_id cannot be empty"):
        Neo4jMapStorage("", "")


async def test_neo4j_map_storage_write_read_finalize(clean_test_database, clean_event_store):
    """
    M2.8: Tests writing via events, projection service processing, and reading from Neo4j.

    Flow:
    1. write_entry() emits SentenceCreated events
    2. Events processed through projection service handlers
    3. Projection service writes to Neo4j
    4. read_all_entries() reads from Neo4j
    """
    project_id = "test-project-write-read"
    interview_id = str(uuid.uuid4())  # Use UUID to avoid stream conflicts

    # Create event emitter
    event_emitter = PipelineEventEmitter(clean_event_store)

    # Emit InterviewCreated event first
    await event_emitter.emit_interview_created(
        interview_id=interview_id,
        project_id=project_id,
        title="test_interview",
        source="/test/path",
    )

    # Create storage with event emitter
    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    entries: List[Dict[str, Any]] = [
        {"sentence_id": 0, "sequence_order": 0, "sentence": "Sentence zero."},
        {"sentence_id": 1, "sequence_order": 1, "sentence": "Sentence one."},
        {"sentence_id": 2, "sequence_order": 2, "sentence": "Sentence two."},
    ]
    # M2.8: sentence_ids are UUIDs generated by projection service
    expected_ids: Set[str] = {
        str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:0")),
        str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:1")),
        str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:2")),
    }

    # Initialize and write (emits events)
    await storage.initialize()
    for entry in entries:
        await storage.write_entry(entry)
    await storage.finalize()

    # M2.8: Process events through projection service
    await process_events_through_projection(clean_event_store, interview_id, len(entries))

    # Read all entries (should be ordered by sequence_order)
    read_entries = await storage.read_all_entries()
    assert len(read_entries) == len(entries), f"Expected {len(entries)} entries, got {len(read_entries)}"

    # Verify the entries match
    for i, entry in enumerate(entries):
        read_entry = read_entries[i]
        # M2.8: sentence_id is now a UUID (from projection service), not the original integer
        # Verify it exists and is a string UUID
        assert "sentence_id" in read_entry
        assert isinstance(read_entry["sentence_id"], str)
        # Compute expected UUID: uuid5(NAMESPACE_DNS, f"{interview_id}:{index}")
        expected_sentence_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:{i}"))
        assert read_entry["sentence_id"] == expected_sentence_id
        assert read_entry["sequence_order"] == entry["sequence_order"]
        assert read_entry["text"] == entry["sentence"]  # Neo4j stores as "text"

    # Read sentence IDs
    read_ids = await storage.read_sentence_ids()
    assert read_ids == expected_ids


async def test_neo4j_map_storage_with_optional_fields(clean_test_database, clean_event_store):
    """M2.8: Tests writing entries with optional fields like start_time, end_time, speaker."""
    project_id = "test-project-optional"
    interview_id = str(uuid.uuid4())

    event_emitter = PipelineEventEmitter(clean_event_store)

    # Emit InterviewCreated
    await event_emitter.emit_interview_created(
        interview_id=interview_id,
        project_id=project_id,
        title="test_interview",
        source="/test/path",
    )

    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    # M2.8: start_time/end_time must be in milliseconds (int) for event emission
    entries: List[Dict[str, Any]] = [
        {
            "sentence_id": 0,
            "sequence_order": 0,
            "sentence": "Hello world.",
            "start_time": 0,  # milliseconds
            "end_time": 2500,  # milliseconds
            "speaker": "John",
        },
        {
            "sentence_id": 1,
            "sequence_order": 1,
            "sentence": "How are you?",
            "start_time": 3000,  # milliseconds
            "end_time": 4800,  # milliseconds
            "speaker": "Jane",
        },
    ]

    await storage.initialize()
    for entry in entries:
        await storage.write_entry(entry)
    await storage.finalize()

    # M2.8: Process events through projection service
    await process_events_through_projection(clean_event_store, interview_id, len(entries))

    # Read back and verify optional fields
    read_entries = await storage.read_all_entries()
    assert len(read_entries) == 2

    for i, original_entry in enumerate(entries):
        read_entry = read_entries[i]
        # M2.8: sentence_id is now a UUID, not the original integer
        expected_sentence_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:{i}"))
        assert read_entry["sentence_id"] == expected_sentence_id
        # M2.8: start_time/end_time already in milliseconds, stored as start_ms/end_ms
        assert read_entry.get("start_ms") == original_entry["start_time"]
        assert read_entry.get("end_ms") == original_entry["end_time"]
        assert read_entry["speaker"] == original_entry["speaker"]


async def test_neo4j_map_storage_initialize_clears_old_data(clean_test_database, clean_event_store):
    """M2.8: Tests that initializing clears old sentences for the same interview."""
    project_id = "test-project-clear"
    interview_id = str(uuid.uuid4())

    event_emitter = PipelineEventEmitter(clean_event_store)

    # Emit InterviewCreated
    await event_emitter.emit_interview_created(
        interview_id=interview_id,
        project_id=project_id,
        title="test_interview",
        source="/test/path",
    )

    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    # First batch of entries
    first_entries: List[Dict[str, Any]] = [
        {"sentence_id": 0, "sequence_order": 0, "sentence": "Old sentence 0."},
        {"sentence_id": 1, "sequence_order": 1, "sentence": "Old sentence 1."},
    ]

    await storage.initialize()
    for entry in first_entries:
        await storage.write_entry(entry)
    await storage.finalize()

    # M2.8: Process events through projection service
    await process_events_through_projection(clean_event_store, interview_id, len(first_entries))

    # Verify first entries exist
    read_entries = await storage.read_all_entries()
    assert len(read_entries) == 2

    # M2.8 Note: initialize() in projection-based architecture doesn't clear old data
    # The projection service is idempotent - replaying events will update existing nodes
    # To truly "clear and re-initialize", you'd delete the interview node and replay events
    # For now, we'll test that new writes create new sentence nodes with different IDs

    # Use different interview_id to simulate fresh initialization
    interview_id_2 = str(uuid.uuid4())

    await event_emitter.emit_interview_created(
        interview_id=interview_id_2,
        project_id=project_id,
        title="test_interview_2",
        source="/test/path",
    )

    storage2 = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id_2,
        event_emitter=event_emitter,
    )

    new_entries: List[Dict[str, Any]] = [
        {"sentence_id": 10, "sequence_order": 0, "sentence": "New sentence 10."},
    ]

    await storage2.initialize()
    for entry in new_entries:
        await storage2.write_entry(entry)
    await storage2.finalize()

    # Process new interview events
    await process_events_through_projection(clean_event_store, interview_id_2, len(new_entries))

    # Verify only new entries exist for new interview
    read_entries_2 = await storage2.read_all_entries()
    assert len(read_entries_2) == 1
    # M2.8: sentence_id is a UUID based on interview_id and sequence_order
    expected_sentence_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id_2}:0"))
    assert read_entries_2[0]["sentence_id"] == expected_sentence_id
    assert read_entries_2[0]["text"] == "New sentence 10."

    # Verify old entries still exist for old interview
    read_entries_1 = await storage.read_all_entries()
    assert len(read_entries_1) == 2


async def test_neo4j_map_storage_missing_required_keys(clean_test_database, clean_event_store):
    """Tests that write_entry raises ValueError for missing required keys."""
    project_id = "test-project-missing"
    interview_id = str(uuid.uuid4())

    event_emitter = PipelineEventEmitter(clean_event_store)
    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    await storage.initialize()

    # Missing sentence_id
    with pytest.raises(ValueError, match="Entry dict missing required keys"):
        await storage.write_entry({"sequence_order": 0, "sentence": "test"})

    # Missing sequence_order
    with pytest.raises(ValueError, match="Entry dict missing required keys"):
        await storage.write_entry({"sentence_id": 0, "sentence": "test"})

    # Missing sentence
    with pytest.raises(ValueError, match="Entry dict missing required keys"):
        await storage.write_entry({"sentence_id": 0, "sequence_order": 0})


async def test_neo4j_map_storage_read_empty_interview(clean_test_database):
    """Tests reading from an interview that has no sentences."""
    project_id = "test-project-empty"
    interview_id = str(uuid.uuid4())
    storage = Neo4jMapStorage(project_id, interview_id)

    # Don't write any entries, just try to read
    read_entries = await storage.read_all_entries()
    assert read_entries == []

    read_ids = await storage.read_sentence_ids()
    assert read_ids == set()


async def test_neo4j_map_storage_multiple_projects(clean_test_database, clean_event_store):
    """M2.8: Tests that different projects/interviews are isolated."""
    event_emitter = PipelineEventEmitter(clean_event_store)

    # Project 1, Interview 1
    interview_id_1 = str(uuid.uuid4())
    await event_emitter.emit_interview_created(
        interview_id=interview_id_1,
        project_id="project-1",
        title="interview-1",
        source="/test/path",
    )

    storage1 = Neo4jMapStorage("project-1", interview_id_1, event_emitter=event_emitter)
    entries1: List[Dict[str, Any]] = [
        {"sentence_id": 0, "sequence_order": 0, "sentence": "Project 1 sentence."},
    ]

    # Project 2, Interview 2
    interview_id_2 = str(uuid.uuid4())
    await event_emitter.emit_interview_created(
        interview_id=interview_id_2,
        project_id="project-2",
        title="interview-2",
        source="/test/path",
    )

    storage2 = Neo4jMapStorage("project-2", interview_id_2, event_emitter=event_emitter)
    entries2: List[Dict[str, Any]] = [
        {"sentence_id": 0, "sequence_order": 0, "sentence": "Project 2 sentence."},
    ]

    # Write to both
    await storage1.initialize()
    await storage1.write_entry(entries1[0])
    await storage1.finalize()

    await storage2.initialize()
    await storage2.write_entry(entries2[0])
    await storage2.finalize()

    # Process events for both interviews
    await process_events_through_projection(clean_event_store, interview_id_1, 1)
    await process_events_through_projection(clean_event_store, interview_id_2, 1)

    # Read from both - should be isolated
    read1 = await storage1.read_all_entries()
    read2 = await storage2.read_all_entries()

    assert len(read1) == 1
    assert len(read2) == 1
    assert read1[0]["text"] == "Project 1 sentence."
    assert read2[0]["text"] == "Project 2 sentence."


async def test_neo4j_map_storage_sequence_ordering(clean_test_database, clean_event_store):
    """M2.8: Tests that entries are returned in correct sequence order."""
    project_id = "test-project-order"
    interview_id = str(uuid.uuid4())

    event_emitter = PipelineEventEmitter(clean_event_store)

    await event_emitter.emit_interview_created(
        interview_id=interview_id,
        project_id=project_id,
        title="test_interview",
        source="/test/path",
    )

    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    # Write entries out of order
    entries: List[Dict[str, Any]] = [
        {"sentence_id": 2, "sequence_order": 2, "sentence": "Third sentence."},
        {"sentence_id": 0, "sequence_order": 0, "sentence": "First sentence."},
        {"sentence_id": 1, "sequence_order": 1, "sentence": "Second sentence."},
    ]

    await storage.initialize()
    for entry in entries:
        await storage.write_entry(entry)
    await storage.finalize()

    # Process events
    await process_events_through_projection(clean_event_store, interview_id, len(entries))

    # Read back - should be ordered by sequence_order
    read_entries = await storage.read_all_entries()
    assert len(read_entries) == 3
    assert read_entries[0]["sequence_order"] == 0
    assert read_entries[0]["text"] == "First sentence."
    assert read_entries[1]["sequence_order"] == 1
    assert read_entries[1]["text"] == "Second sentence."
    assert read_entries[2]["sequence_order"] == 2
    assert read_entries[2]["text"] == "Third sentence."


async def test_neo4j_map_storage_non_integer_sentence_ids(clean_test_database, clean_event_store):
    """M2.8: Tests handling of non-integer sentence IDs in read_sentence_ids."""
    project_id = "test-project-nonint"
    interview_id = str(uuid.uuid4())

    event_emitter = PipelineEventEmitter(clean_event_store)

    await event_emitter.emit_interview_created(
        interview_id=interview_id,
        project_id=project_id,
        title="test_interview",
        source="/test/path",
    )

    storage = Neo4jMapStorage(
        project_id=project_id,
        interview_id=interview_id,
        event_emitter=event_emitter,
    )

    entries: List[Dict[str, Any]] = [
        {"sentence_id": 0, "sequence_order": 0, "sentence": "Test sentence."},
        {"sentence_id": 42, "sequence_order": 1, "sentence": "Another sentence."},
    ]

    await storage.initialize()
    for entry in entries:
        await storage.write_entry(entry)
    await storage.finalize()

    # Process events
    await process_events_through_projection(clean_event_store, interview_id, len(entries))

    read_ids = await storage.read_sentence_ids()
    # M2.8: sentence_ids are UUIDs based on interview_id and sequence_order
    expected_id_0 = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:0"))
    expected_id_1 = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{interview_id}:1"))
    assert read_ids == {expected_id_0, expected_id_1}
